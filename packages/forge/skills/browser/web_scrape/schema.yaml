skill_name: web_scrape
version: 1.0.0
agent: browser
author: Gorgon Project
license: MIT

description: |
  Fetch and extract content from web pages. Parse HTML, extract text, 
  tables, and structured data. Handle JavaScript-rendered content with 
  headless browser.

dependencies:
  required:
    - python3
    - requests
    - beautifulsoup4
  optional:
    - playwright      # For JavaScript rendering
    - pandas          # For table extraction
    - lxml            # Faster parsing

capabilities:
  fetch_page:
    description: Retrieve a web page's HTML content
    inputs:
      url:
        type: string
        required: true
        description: URL to fetch
        pattern: "^https?://"
      timeout:
        type: integer
        required: false
        default: 30
        description: Request timeout in seconds
      headers:
        type: object
        required: false
        description: Custom headers to send
      use_javascript:
        type: boolean
        required: false
        default: false
        description: Use headless browser for JS rendering
      wait_for_selector:
        type: string
        required: false
        description: CSS selector to wait for (JS mode only)
    outputs:
      success: boolean
      url: string
      status_code: integer
      content_type: string
      html: string
      cached: boolean
    risk_level: low
    consensus_required: any

  extract_text:
    description: Extract readable text content from HTML
    inputs:
      html:
        type: string
        required: true
        description: HTML content to parse
      selector:
        type: string
        required: false
        description: CSS selector to extract from
      remove_elements:
        type: array[string]
        required: false
        default: ["script", "style", "nav", "footer"]
        description: Elements to remove before extraction
    outputs:
      success: boolean
      text: string
      char_count: integer
      word_count: integer
    risk_level: low
    consensus_required: any

  extract_article:
    description: Extract main article content, removing boilerplate
    inputs:
      html:
        type: string
        required: true
    outputs:
      success: boolean
      title: string
      content: string
      selector_used: string
    risk_level: low
    consensus_required: any

  extract_tables:
    description: Extract tables from HTML into structured data
    inputs:
      html:
        type: string
        required: true
      table_index:
        type: integer
        required: false
        description: Extract only specific table by index
    outputs:
      success: boolean
      table_count: integer
      tables: array[object]
    risk_level: low
    consensus_required: any

  extract_links:
    description: Extract all links from a page
    inputs:
      html:
        type: string
        required: true
      base_url:
        type: string
        required: true
        description: Base URL for resolving relative links
      filter_domain:
        type: boolean
        required: false
        default: false
        description: Only return links to same domain
      filter_pattern:
        type: string
        required: false
        description: Regex pattern to filter URLs
    outputs:
      success: boolean
      link_count: integer
      links: array[object]
    risk_level: low
    consensus_required: any

  extract_metadata:
    description: Extract page metadata (title, description, OpenGraph)
    inputs:
      html:
        type: string
        required: true
      url:
        type: string
        required: true
    outputs:
      success: boolean
      metadata: object
    risk_level: low
    consensus_required: any

  screenshot:
    description: Take a screenshot of a web page
    inputs:
      url:
        type: string
        required: true
      output_path:
        type: string
        required: true
        description: Path to save screenshot
      full_page:
        type: boolean
        required: false
        default: false
      viewport_width:
        type: integer
        required: false
        default: 1920
      viewport_height:
        type: integer
        required: false
        default: 1080
    outputs:
      success: boolean
      output_path: string
    risk_level: low
    consensus_required: any

  check_robots:
    description: Check if URL is allowed by robots.txt
    inputs:
      url:
        type: string
        required: true
      user_agent:
        type: string
        required: false
        default: "GorgonBot"
    outputs:
      success: boolean
      allowed: boolean
      crawl_delay: number | null
    risk_level: low
    consensus_required: any

# Rate limiting configuration
rate_limits:
  default_delay_seconds: 1.0
  max_requests_per_domain_per_minute: 30
  respect_crawl_delay: true

# Cache configuration
cache:
  enabled: true
  ttl_hours: 24
  max_size_mb: 500
  directory: "~/.gorgon/cache/scraper"

# User agent for requests
user_agent: "GorgonBot/1.0 (+https://github.com/yourrepo)"
